{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Recommendations Using Object2Vec on Instacart Data\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Data Exploration and Preparation](#Data-Exploration-and-Preparation)\n",
    "1. [Create Product Recommendation Model](#Create-Product-Recommendation-Model)\n",
    "1. [Product Retrieval in the Embedding Space](#Product-Retrieval-in-the-Embedding-Space)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "### ObjectToVec\n",
    "*Object2Vec* is a highly customizable multi-purpose algorithm that can learn embeddings of pairs of objects. Embeddings are an important feature engineering technique in machine learning (ML). They convert high dimensional vectors into low-dimensional space to make it easier to do machine learning with large sparse vector inputs. Embeddings also capture the semantics of the underlying data by placing similar items closer in the low-dimensional space. This makes the features more effective in training downstream models.\n",
    "\n",
    "One of the well-known embedding techniques is Word2Vec, which provides embeddings for words. It has been widely used in many use cases, such as sentiment analysis, document classification, and natural language understanding. In addition to word embeddings, there are also use cases where we want to learn the embeddings of more general-purpose objects such as sentences, customers, and products. This is so we can build practical applications for information retrieval, product search, item matching, customer profiling based on similarity or as inputs for other supervised tasks. This is where Amazon SageMaker Object2Vec comes in, where the algorithm's embeddings are learned such that it preserves their pairwise **similarities** in the original space.\n",
    "- **Similarity** is user-defined: users need to provide the algorithm with pairs of objects that they define as similar (1) or dissimilar (0); alternatively, the users can define similarity in a continuous sense (provide a real-valued similarity score)\n",
    "\n",
    "- The learned embeddings can be used to efficiently compute nearest neighbors of objects, as well as to visualize natural clusters of related objects in the embedding space. In addition, the embeddings can also be used as features of the corresponding objects in downstream supervised tasks such as classification or regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook example:\n",
    "We demonstrate how Object2Vec can be used to solve problems arising in recommendation systems. Specifically, the diagram below shows the customization of our model to the problem of predicting product recommendations, using a dataset that provides `(UserID, ProductID, Reordered)` samples.\n",
    "\n",
    "#### Training with pairs of tokens: Collaborative recommendation system\n",
    "\n",
    "Collaborative filtering is a popular technique for building recommendation systems. The main concept behind collaborative filtering is that users with similar tastes (based on observed user-item interactions) are more likely to have similar interactions with new items. Object2Vec can make recommendations by approximating the observed user-item interactions using low dimensional representations of users and items.\n",
    "\n",
    "The following diagram shows how user-item interaction data can be used to learn the embedding of users and items. The resulting model can be used to predict user rating on a new item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:middle\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/11/06/sagemaker-object2vec-4.gif\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- We use the Instacart Market Basket dataset: https://www.kaggle.com/c/instacart-market-basket-analysis/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Running the Notebook\n",
    "- Please use a Python 3 kernel for the notebook\n",
    "- Please make sure you have jsonlines package installed (if not, you can run the command below to install it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install jsonlines\n",
    "!pip install --upgrade pip\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define important notebook variables\n",
    "bucket = 'test-notebook-workshop'  #bucket name containing the uploaded Instacart Market Dataset\n",
    "prefix = 'dataset'\n",
    " \n",
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import csv, jsonlines\n",
    "import copy\n",
    "import random\n",
    "import struct\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by uploading the data sets from S3 and importing them as dataframes using pandas. We will then merge the data files and transform the data into a usable format for Object2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main data sets we will utilize:\n",
    "orders_dataFile = 'orders.csv'\n",
    "orders_data_location = 's3://{}/{}'.format(bucket, prefix, orders_dataFile)\n",
    "orders_data = pd.read_csv(orders_data_location)\n",
    "\n",
    "products_dataFile = 'products.csv'\n",
    "products_data_location = 's3://{}/{}'.format(bucket, prefix, products_dataFile)\n",
    "products_data = pd.read_csv(products_data_location)\n",
    "\n",
    "ordersP_dataFile = 'order_products__prior.csv'\n",
    "ordersP_data_location = 's3://{}/{}'.format(bucket, prefix, ordersP_dataFile)\n",
    "ordersP_data = pd.read_csv(ordersP_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge data files\n",
    "orders_merge = pd.merge(ordersP_data,\n",
    "               orders_data[['user_id', 'order_number', 'order_id']],\n",
    "               on ='order_id')\n",
    "products_merge = pd.merge(orders_merge,\n",
    "               products_data[['product_id', 'product_name']],\n",
    "               on ='product_id')\n",
    "products_merge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop unneeded columns\n",
    "drop_data = products_merge.drop(['product_name', 'add_to_cart_order', 'order_number','order_id'], 1)\n",
    "cols = drop_data.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "final_data = drop_data[cols]\n",
    "\n",
    "## For the purposes of this workshop, we will take a subset of the extremely large dataset to use for training.\n",
    "subset_final_data = final_data.head(200000)\n",
    "print(subset_final_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some utility functions for further data exploration and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some utility functions\n",
    "\n",
    "def load_csv_data(filename, delimiter, verbose=True):\n",
    "    \"\"\"\n",
    "    input: a file readable as csv and separated by a delimiter\n",
    "    and has format users - products - reordered - etc\n",
    "    output: a list, where each row of the list is of the form\n",
    "    {'in0':userID, 'in1':productID, 'label':reordered}\n",
    "    \"\"\"\n",
    "    to_data_list = list()\n",
    "    users = list()\n",
    "    products = list()\n",
    "    reordered = list()\n",
    "    unique_users = set()\n",
    "    unique_products = set()\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        for count, row in enumerate(reader):\n",
    "            #if count!=0:\n",
    "            to_data_list.append({'in0':[int(row[0])], 'in1':[int(row[1])], 'label':float(row[2])})\n",
    "            users.append(row[0])\n",
    "            products.append(row[1])\n",
    "            reordered.append(float(row[2]))\n",
    "    if verbose:\n",
    "        print(\"In file {}, there are {} products\".format(filename, len(products)))\n",
    "    return to_data_list\n",
    "\n",
    "\n",
    "def csv_to_augmented_data_dict(filename, delimiter):\n",
    "    \"\"\"\n",
    "    Input: a file that must be readable as csv and separated by delimiter (to make columns)\n",
    "    has format users - products - reordered - etc\n",
    "    Output:\n",
    "      Users dictionary: keys as user ID's; each key corresponds to a list of product bought by that user\n",
    "      Products dictionary: keys as product ID's; each key corresponds a list of products bought by different users\n",
    "    \"\"\"\n",
    "    to_users_dict = dict() \n",
    "    to_products_dict = dict()\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        for count, row in enumerate(reader):\n",
    "            #if count!=0:\n",
    "            if row[0] not in to_users_dict:\n",
    "                to_users_dict[row[0]] = [(row[1], row[2])]\n",
    "            else:\n",
    "                to_users_dict[row[0]].append((row[1], row[2]))\n",
    "            if row[1] not in to_products_dict:\n",
    "                to_products_dict[row[1]] = list(row[0])\n",
    "            else:\n",
    "                to_products_dict[row[1]].append(row[0])\n",
    "    return to_users_dict, to_products_dict\n",
    "\n",
    "\n",
    "def user_dict_to_data_list(user_dict):\n",
    "    # turn user_dict format to data list format (acceptable to the algorithm)\n",
    "    data_list = list()\n",
    "    for user, product_reordered_list in user_dict.items():\n",
    "        for product, reordered in product_reordered_list:\n",
    "            data_list.append({'in0':[int(user)], 'in1':[int(product)], 'label':float(reordered)})\n",
    "    return data_list\n",
    "\n",
    "def divide_user_dicts(user_dict, sp_ratio_dict):\n",
    "    \"\"\"\n",
    "    Input: A user dictionary, a ration dictionary\n",
    "         - format of sp_ratio_dict = {'train':0.8, \"test\":0.2}\n",
    "    Output: \n",
    "        A dictionary of dictionaries, with key corresponding to key provided by sp_ratio_dict\n",
    "        and each key corresponds to a subdivded user dictionary\n",
    "    \"\"\"\n",
    "    ratios = [val for _, val in sp_ratio_dict.items()]\n",
    "    assert np.sum(ratios) == 1, \"the sampling ratios must sum to 1!\"\n",
    "    divided_dict = {}\n",
    "    for user, product_list in user_dict.items():\n",
    "        sub_products_ptr = 0\n",
    "        sub_products_list = []\n",
    "        #mproduct_list, _ = zip(*product_rating_list)\n",
    "        #print(product_list)\n",
    "        for i, ratio in enumerate(ratios):\n",
    "            if i < len(ratios)-1:\n",
    "                sub_products_ptr_end = sub_products_ptr + int(len(product_list)*ratio)\n",
    "                sub_products_list.append(product_list[sub_products_ptr:sub_products_ptr_end])\n",
    "                sub_products_ptr = sub_products_ptr_end\n",
    "            else:\n",
    "                sub_products_list.append(product_list[sub_products_ptr:])\n",
    "        for subset_name in sp_ratio_dict.keys():\n",
    "            if subset_name not in divided_dict:\n",
    "                divided_dict[subset_name] = {user: sub_products_list.pop(0)}\n",
    "            else:\n",
    "                #access sub-dictionary\n",
    "                divided_dict[subset_name][user] = sub_products_list.pop(0)\n",
    "    \n",
    "    return divided_dict\n",
    "\n",
    "def write_csv_to_jsonl(jsonl_fname, csv_fname, csv_delimiter):\n",
    "    \"\"\"\n",
    "    Input: a file readable as csv and separated by delimiter (to make columns)\n",
    "        - has format users - products - reordered - etc\n",
    "    Output: a jsonline file converted from the csv file\n",
    "    \"\"\"\n",
    "    with jsonlines.open(jsonl_fname, mode='w') as writer:\n",
    "        with open(csv_fname, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=csv_delimiter)\n",
    "            for count, row in enumerate(reader):\n",
    "                #print(row)\n",
    "                #if count!=0:\n",
    "                writer.write({'in0':[int(row[0])], 'in1':[int(row[1])], 'label':float(row[2])})\n",
    "        print('Created {} jsonline file'.format(jsonl_fname))\n",
    "                    \n",
    "    \n",
    "def write_data_list_to_jsonl(data_list, to_fname):\n",
    "    \"\"\"\n",
    "    Input: a data list, where each row of the list is a Python dictionary taking form\n",
    "    {'in0':userID, 'in1':productID, 'label':reordered}\n",
    "    Output: save the list as a jsonline file\n",
    "    \"\"\"\n",
    "    with jsonlines.open(to_fname, mode='w') as writer:\n",
    "        for row in data_list:\n",
    "            #print(row)\n",
    "            writer.write({'in0':row['in0'], 'in1':row['in1'], 'label':row['label']})\n",
    "    print(\"Created {} jsonline file\".format(to_fname))\n",
    "\n",
    "def data_list_to_inference_format(data_list, binarize=True, label_thres=0.5):\n",
    "    \"\"\"\n",
    "    Input: a data list\n",
    "    Output: test data and label, acceptable by SageMaker for inference\n",
    "    \"\"\"\n",
    "    data_ = [({\"in0\":row['in0'], 'in1':row['in1']}, row['label']) for row in data_list]\n",
    "    data, label = zip(*data_)\n",
    "    infer_data = {\"instances\":data}\n",
    "    if binarize:\n",
    "        label = get_binarized_label(list(label), label_thres)\n",
    "    return infer_data, label\n",
    "\n",
    "\n",
    "def get_binarized_label(data_list, thres):\n",
    "    \"\"\"\n",
    "    Input: data list\n",
    "    Output: a binarized data list for recommendation task\n",
    "    \"\"\"\n",
    "    for i, row in enumerate(data_list):\n",
    "        if type(row) is dict:\n",
    "            #if i < 10:\n",
    "                #print(row['label'])\n",
    "            if row['label'] > thres:\n",
    "                #print(row)\n",
    "                data_list[i]['label'] = 1\n",
    "            else:\n",
    "                data_list[i]['label'] = 0\n",
    "        else:\n",
    "            if row > thres:\n",
    "                data_list[i] = 1\n",
    "            else:\n",
    "                data_list[i] = 0\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data and split into test/train/val\n",
    "\n",
    "y = subset_final_data.product_id \n",
    "X = subset_final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export Data to S3\n",
    "\n",
    "training_data_bucket = 'test-notebook-workshop'  #your bucket name that you created earlier in the console\n",
    "\n",
    "from io import StringIO\n",
    "DESTINATION = training_data_bucket\n",
    "\n",
    "def _write_dataframe_to_csv_on_s3(dataframe, filename):\n",
    "    #\"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    print(\"Writing {} records to {}\".format(len(dataframe), filename))\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    dataframe.to_csv(csv_buffer, sep=\"\\t\", index=False, header=False)\n",
    "    # Create S3 object\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(DESTINATION, filename).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "_write_dataframe_to_csv_on_s3(X_train, 'final_training_data')\n",
    "_write_dataframe_to_csv_on_s3(X_val, 'final_validation_data')\n",
    "_write_dataframe_to_csv_on_s3(products_data, 'products_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sync files from S3 to local SageMaker instance\n",
    "\n",
    "#enter your bucket name from above = training_data_bucket\n",
    "!aws s3 sync s3://retail-analytics-workshop-<your-initials> {training-data} #your bucket name from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data and shuffle:\n",
    "\n",
    "prefix = '/home/ec2-user/SageMaker/{training-data}'\n",
    "train_path = os.path.join(prefix, 'final_training_data')\n",
    "val_path = os.path.join(prefix, 'final_validation_data')\n",
    "\n",
    "train_data_list = load_csv_data(train_path, '\\t')\n",
    "random.shuffle(train_data_list)\n",
    "validation_data_list = load_csv_data(val_path, '\\t')\n",
    "random.shuffle(validation_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_users_dict, to_products_dict = csv_to_augmented_data_dict(train_path, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save training and validation data locally for recommendation (classification) task\n",
    "\n",
    "### Binarize the data\n",
    "\n",
    "train_c = get_binarized_label(copy.deepcopy(train_data_list), 3.0)\n",
    "valid_c = get_binarized_label(copy.deepcopy(validation_data_list), 3.0)\n",
    "\n",
    "write_data_list_to_jsonl(train_c, 'train_c.jsonl')\n",
    "write_data_list_to_jsonl(valid_c, 'validation_c.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Product Recommendation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we showcase how to use Object2Vec to recommend products. Here, if a product for a given user is binarized to 1, then it means that the product should be recommended to the user; otherwise, the label is binarized to 0. The binarized data set is already obtained in the preprocessing section, so we will proceed to apply the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3\n",
    "We upload the binarized datasets for classification task to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = 'object2vec/retail/input'\n",
    "output_prefix = 'object2vec/retail/output'\n",
    "\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "input_paths = {}\n",
    "output_path = os.path.join('s3://', training_data_bucket, output_prefix)\n",
    "\n",
    "for data_name in ['train', 'validation']:\n",
    "    fname = '{}_c.jsonl'.format(data_name)\n",
    "    pre_key = os.path.join(input_prefix, 'recommendation', f\"{data_name}\")\n",
    "    data_path = os.path.join('s3://', training_data_bucket, pre_key, fname)\n",
    "    s3_client.upload_file(fname, training_data_bucket, os.path.join(pre_key, fname))\n",
    "    input_paths[data_name] = s3_input(data_path, distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "    print('Uploaded data to {}'.format(data_path))\n",
    "\n",
    "print('Trained model will be saved at', output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ObjectToVec Algorithm Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get obj2vec image\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "## Get docker image of ObjectToVec algorithm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'object2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define training hyperparameters. To learn more about SageMaker's Object2Vec hyperparameters, please visit our documentation page:\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "## Define Object2Vec hyperparameters:\n",
    "\n",
    "hyperparameters_c = {\n",
    "    \"_kvstore\": \"device\",\n",
    "    \"_num_gpus\": \"auto\",\n",
    "    \"_num_kv_servers\": \"auto\",\n",
    "    \"bucket_width\": 0,\n",
    "    \"early_stopping_patience\": 1, \n",
    "    \"early_stopping_tolerance\": 0.01,\n",
    "    \"enc0_cnn_filter_width\": 3,\n",
    "    \"enc0_layers\": \"auto\",\n",
    "    \"enc0_max_seq_len\": 1,\n",
    "    \"enc0_network\": \"pooled_embedding\",\n",
    "    \"enc0_token_embedding_dim\": 300,\n",
    "    \"enc0_vocab_size\": 220000,\n",
    "    \"enc1_cnn_filter_width\": 3,\n",
    "    \"enc1_layers\": \"auto\",\n",
    "    \"enc1_max_seq_len\": 1,\n",
    "    \"enc1_network\": \"pooled_embedding\",\n",
    "    \"enc1_token_embedding_dim\": 300,\n",
    "    \"enc1_vocab_size\": 220000,\n",
    "    \"enc_dim\": 2048,\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"mini_batch_size\": 200,\n",
    "    \"mlp_activation\": \"relu\",\n",
    "    \"mlp_dim\": 1024,\n",
    "    \"mlp_layers\": 1,\n",
    "    \"num_classes\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"output_layer\": \"softmax\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get estimator\n",
    "classifier = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.24xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "## Set hyperparameters\n",
    "classifier.set_hyperparameters(**hyperparameters_c)\n",
    "\n",
    "## Train, tune, and test the model (training runtime: ~25 min)\n",
    "classifier.fit(input_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create, deploy, and validate the model after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "## Create a model using the trained algorithm\n",
    "classification_model = classifier.create_model(\n",
    "                        serializer=json_serializer,\n",
    "                        deserializer=json_deserializer,\n",
    "                        content_type='application/json')\n",
    "\n",
    "## Deploy the model (~12 min.)\n",
    "predictor = classification_model.deploy(initial_instance_count=1, instance_type='ml.m5.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_c_data, valid_c_label = data_list_to_inference_format(copy.deepcopy(validation_data_list), \n",
    "                                                            label_thres=0.5, binarize=True)\n",
    "predictions = predictor.predict(valid_c_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_accuracy(res, labels, thres):\n",
    "    if type(res) is dict:\n",
    "        res = res['predictions']\n",
    "    assert len(res)==len(labels), 'result and label length mismatch!'\n",
    "    accuracy = 0\n",
    "    for row, label in zip(res, labels):\n",
    "        if type(row) is dict:\n",
    "            if row['scores'][1] > thres:\n",
    "                prediction = 1\n",
    "            else: \n",
    "                prediction = 0\n",
    "            if label > thres:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            accuracy += 1 - (prediction - label)**2\n",
    "    return accuracy / float(len(res))\n",
    "\n",
    "print(\"The accuracy on the binarized validation set is %.3f\" %get_class_accuracy(predictions, valid_c_label, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Retrieval in the Embedding Space\n",
    "Since Object2Vec transforms user and product ID's into embeddings as part of the training process - after training, it obtains user and product embeddings in the left and right encoders, respectively. Intuitively, the embeddings should be tuned by the algorithm in a way that facilitates the supervised learning task: since for a specific user, popular products should have been reordered, we expect that similar products that users would be interested in buying should be **close-by** in the embedding space.\n",
    "\n",
    "In this section, we demonstrate how to find the nearest-neighbor (in Euclidean distance) of a given product ID, among all product ID's in our subset of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some utility functions for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_embedding_dict(product_ids, trained_model):\n",
    "    input_instances = list()\n",
    "    for s_id in product_ids:\n",
    "        input_instances.append({'in1': [s_id]})\n",
    "    data = {'instances': input_instances}\n",
    "    product_embeddings = trained_model.predict(data)\n",
    "    embedding_dict = {}\n",
    "    for s_id, row in zip(product_ids, product_embeddings['predictions']):\n",
    "        embedding_dict[s_id] = np.array(row['embeddings'])\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "def get_nn_of_product(product_id, candidate_product_ids, embedding_dict):\n",
    "    product_emb = embedding_dict[product_id]\n",
    "    min_dist = float('Inf')\n",
    "    best_id = candidate_product_ids[0]\n",
    "    for idx, m_id in enumerate(candidate_product_ids):\n",
    "        candidate_emb = embedding_dict[m_id]\n",
    "        curr_dist = np.linalg.norm(candidate_emb - product_emb)\n",
    "        if curr_dist < min_dist:\n",
    "            best_id = m_id\n",
    "            min_dist = curr_dist\n",
    "    return best_id, min_dist\n",
    "\n",
    "\n",
    "def get_unique_product_ids(data_list):\n",
    "    unique_product_ids = set()\n",
    "    for row in data_list:\n",
    "        unique_product_ids.add(row['in1'][0])\n",
    "    return list(unique_product_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = load_csv_data(train_path, '\\t', verbose=False)\n",
    "unique_product_ids = get_unique_product_ids(train_data_list)\n",
    "embedding_dict = get_product_embedding_dict(unique_product_ids, predictor)\n",
    "candidate_product_ids = unique_product_ids.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the script below, you can check out what is the closest product to any product in the data set. Last time we ran it, the closest product to Carrots in the embedding space was Organic Kale. Note that, the result will likely differ slightly across different runs of the algorithm, due to randomness in initialization of model parameters.\n",
    "\n",
    "However, let's plug in the product id for Carrots, 17794, that we want to examine and validate this prior recommendation (you can find the product name and ID pair in the products_data file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product_id_to_examine = '<product id for carrots>'\n",
    "product_id_to_examine = 17794\n",
    "products_data.loc[products_data['product_id'] == 17794]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the nearest neighbor to Carrots in the embedding space and find the product's name from our dataset using it's product ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/ec2-user/SageMaker/{training-data}'\n",
    "product_data_path = os.path.join(prefix, 'final_training_data')\n",
    "candidate_product_ids.remove(product_id_to_examine)\n",
    "best_id, min_dist = get_nn_of_product(product_id_to_examine, candidate_product_ids, embedding_dict)\n",
    "products_data.loc[products_data['product_id'] == best_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can see that our model has correctly recommended to buy Michigan Organic Kale, for those users who have bought Carrots in the past. Both being vegetables, we can understand why we would recommend Kale to customers who previously bought Carrots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to always delete the endpoints used for hosting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete model endpoint\n",
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
